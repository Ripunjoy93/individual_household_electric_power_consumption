{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model using univariate data to vector output\n",
    "**Going to aggregate the data into hourly basis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "# !/usr/bin/env python3\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset, if file is not available run: 1_Exploratory_Analylis.ipynb\n",
    "df = pd.read_csv(\"./data/cleaned_household_power_consumption.csv\", infer_datetime_format=True, parse_dates=[\"local_time\"],\n",
    "                index_col=[\"local_time\"], dtype=np.float32)\n",
    "# Just to be safer, data is already sorted\n",
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Range:  2006-12-16 17:24:00  to  2010-11-26 21:02:00\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Range: \", df.index.min(), \" to \", df.index.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Not in scope:**    \n",
    "As power is instantaneous reading, we will convert power to energy, and will predict the energy consumption in hours.\n",
    "Ideally global_active_power is in kw, sub meters are in watt-hours, need convert this power to energy also the unit to watt-hour, not kw-hour.     \n",
    "_**But just to showcase how models can be build, we won't go much into physics and aggregate accordingly, rather will just keep power as power and energy as energy for now**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.resample(\"H\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>global_active_power</th>\n",
       "      <th>global_reactive_power</th>\n",
       "      <th>voltage</th>\n",
       "      <th>global_intensity</th>\n",
       "      <th>sub_metering_1</th>\n",
       "      <th>sub_metering_2</th>\n",
       "      <th>sub_metering_3</th>\n",
       "      <th>sub_metering_other</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-12-16 17:00:00</th>\n",
       "      <td>152.024002</td>\n",
       "      <td>8.243999</td>\n",
       "      <td>8447.179688</td>\n",
       "      <td>651.599976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>607.0</td>\n",
       "      <td>1907.733398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-16 18:00:00</th>\n",
       "      <td>217.932007</td>\n",
       "      <td>4.802000</td>\n",
       "      <td>14074.809570</td>\n",
       "      <td>936.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>2217.199951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-16 19:00:00</th>\n",
       "      <td>204.013992</td>\n",
       "      <td>5.114000</td>\n",
       "      <td>13993.950195</td>\n",
       "      <td>870.200012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>2313.233398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-16 20:00:00</th>\n",
       "      <td>196.113998</td>\n",
       "      <td>4.506000</td>\n",
       "      <td>14044.290039</td>\n",
       "      <td>835.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>2261.566650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-16 21:00:00</th>\n",
       "      <td>183.388000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>14229.519531</td>\n",
       "      <td>782.799988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1033.0</td>\n",
       "      <td>1998.466675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     global_active_power  global_reactive_power       voltage  \\\n",
       "local_time                                                                      \n",
       "2006-12-16 17:00:00           152.024002               8.243999   8447.179688   \n",
       "2006-12-16 18:00:00           217.932007               4.802000  14074.809570   \n",
       "2006-12-16 19:00:00           204.013992               5.114000  13993.950195   \n",
       "2006-12-16 20:00:00           196.113998               4.506000  14044.290039   \n",
       "2006-12-16 21:00:00           183.388000               4.600000  14229.519531   \n",
       "\n",
       "                     global_intensity  sub_metering_1  sub_metering_2  \\\n",
       "local_time                                                              \n",
       "2006-12-16 17:00:00        651.599976             0.0            19.0   \n",
       "2006-12-16 18:00:00        936.000000             0.0           403.0   \n",
       "2006-12-16 19:00:00        870.200012             0.0            86.0   \n",
       "2006-12-16 20:00:00        835.000000             0.0             0.0   \n",
       "2006-12-16 21:00:00        782.799988             0.0            25.0   \n",
       "\n",
       "                     sub_metering_3  sub_metering_other  \n",
       "local_time                                               \n",
       "2006-12-16 17:00:00           607.0         1907.733398  \n",
       "2006-12-16 18:00:00          1012.0         2217.199951  \n",
       "2006-12-16 19:00:00          1001.0         2313.233398  \n",
       "2006-12-16 20:00:00          1007.0         2261.566650  \n",
       "2006-12-16 21:00:00          1033.0         1998.466675  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we can see the first and last records don't have all the minutes, moreover the last records have only 2 minutes of data. So, I will get rid of this two records**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.index.isin([\"2006-12-16 17:00:00\", \"2010-11-26 21:00:00\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34587, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Range:  2006-12-16 18:00:00  to  2010-11-26 20:00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Range: \", df.index.min(), \" to \", df.index.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategies for hourly prediction (next 48 hours)\n",
    "1. We will use sequence of **global_active_power** values to predict **global_active_power** itself. It's an univariate model.\n",
    "2. We need to provide multi-step time forecast (asking for minutely forecast for 2 days ie. 2 * 24 data points). Data start and end is not from begining hour of the first day also not till end hour of last day.\n",
    "3. For new prediction purpose, we will give 2 days prediction. That is 2010-11-24 21:00:00 to 2010-11-24 20:00:00.\n",
    "4. Model can be trained using prior few days/hours data (here equivalent hours). We will start with using prior 2 days data (48 hours data) to train the model. It can be extended to any days, say 3, 4, 5 days. or if we look through hours, say 72, 100, 120 hours prior.\n",
    "5. We will split the data into 3 sets (train, evaluate, test). And it will be in time series. And we will create a distict boundaries that no set can see the data from other set (be it the input sequence to train the model). We will keep last 48 + number of data in training as train set (if we don't do this, data will overlap). Here for test data, now we have 96 data points (considering, training with 48 data points). We will evaluate our model with 30 days (720 hours), so will keep next last 32 days data (768 hours). And remaining data points we will use for training our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Evaluate & Test split of univariate data:\n",
    "**(I am not creating functions now, once we are clear and confident about our steps we can wrap different building blocks as functions)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (33723, 8)\n",
      "Evaluation data shape:  (768, 8)\n",
      "Test data shape:  (96, 8)\n"
     ]
    }
   ],
   "source": [
    "n_input = 2 * 24\n",
    "n_output = 2 * 24\n",
    "test_index_start = n_input + n_output\n",
    "# 30 evaluation days\n",
    "eval_days = 30\n",
    "eval_index_start = n_input + eval_days * 24 + test_index_start\n",
    "values = df.values\n",
    "train, evaluate, test = values[:-eval_index_start], values[-eval_index_start:-test_index_start], values[-test_index_start:]\n",
    "print(\"Training data shape: \", train.shape)\n",
    "print(\"Evaluation data shape: \", evaluate.shape)\n",
    "print(\"Test data shape: \", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We are getting evaluation and test data as mentioned in our startegy**    \n",
    "Note: Still we have 8 features, we need only 1 feature (global_active_power in index 0) to build our univariate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing:     \n",
    "(I am not trying to make a pipeline here. Later while productionizing the model pipeline will make more sense).   \n",
    "**Missing values imputation is already done, here I will to scale the data**    \n",
    "Note: We can experiment with and without scaling, but for simplicity and faster learning will scale the features through out the project.\n",
    "_I am going to use same strategy for multi-variate approach also as well as for different set of moddels. So, I will be saving the fitted scalar with all these 8 features for later use, though here I am using only 1 feature._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training scaled data shape:  (33723, 8)\n",
      "Evaluation scaled data shape:  (768, 8)\n",
      "Test scaled data shape:  (96, 8)\n"
     ]
    }
   ],
   "source": [
    "scalar = MinMaxScaler()\n",
    "train_scaled = scalar.fit_transform(X=train)\n",
    "eval_scaled = scalar.transform(X=evaluate)\n",
    "test_scaled = scalar.transform(X=test)\n",
    "print(\"Training scaled data shape: \", train_scaled.shape)\n",
    "print(\"Evaluation scaled data shape: \", eval_scaled.shape)\n",
    "print(\"Test scaled data shape: \", test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./hourly_models/min_max_all_feat_48_to_48.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving this object for later use\n",
    "joblib.dump(scalar, \"./hourly_models/min_max_all_feat_48_to_48.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving train, eval, test for later use\n",
    "np.save(\"./hourly_models/train_scaled_48_to_48.npy\", train_scaled)\n",
    "np.save(\"./hourly_models/eval_scaled_48_to_48.npy\", eval_scaled)\n",
    "np.save(\"./hourly_models/test_scaled_48_to_48.npy\", test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable             Type            Data/Info\n",
      "----------------------------------------------\n",
      "MinMaxScaler         type            <class 'sklearn.preprocessing.data.MinMaxScaler'>\n",
      "df                   DataFrame                            glob<...>n[34587 rows x 8 columns]\n",
      "eval_days            int             30\n",
      "eval_index_start     int             864\n",
      "eval_scaled          ndarray         768x8: 6144 elems, type `float32`, 24576 bytes\n",
      "evaluate             ndarray         768x8: 6144 elems, type `float32`, 24576 bytes\n",
      "joblib               module          <module 'sklearn.external<...>ls\\\\joblib\\\\__init__.py'>\n",
      "mean_squared_error   function        <function mean_squared_er<...>or at 0x0000026EEB651D08>\n",
      "n_input              int             48\n",
      "n_output             int             48\n",
      "np                   module          <module 'numpy' from 'c:\\<...>ges\\\\numpy\\\\__init__.py'>\n",
      "pd                   module          <module 'pandas' from 'c:<...>es\\\\pandas\\\\__init__.py'>\n",
      "plt                  module          <module 'matplotlib.pyplo<...>\\\\matplotlib\\\\pyplot.py'>\n",
      "scalar               MinMaxScaler    MinMaxScaler(copy=True, feature_range=(0, 1))\n",
      "test                 ndarray         96x8: 768 elems, type `float32`, 3072 bytes\n",
      "test_index_start     int             96\n",
      "test_scaled          ndarray         96x8: 768 elems, type `float32`, 3072 bytes\n",
      "tf                   module          <module 'tensorflow' from<...>tensorflow\\\\__init__.py'>\n",
      "train                ndarray         33723x8: 269784 elems, type `float32`, 1079136 bytes (1.029144287109375 Mb)\n",
      "train_scaled         ndarray         33723x8: 269784 elems, type `float32`, 1079136 bytes (1.029144287109375 Mb)\n",
      "values               ndarray         34587x8: 276696 elems, type `float32`, 1106784 bytes (1.055511474609375 Mb)\n",
      "warnings             module          <module 'warnings' from '<...>hon37\\\\lib\\\\warnings.py'>\n"
     ]
    }
   ],
   "source": [
    "# Check the available variables and free up memory\n",
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory % used: 49.6\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "print('memory % used:', psutil.virtual_memory()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting variables no longer required\n",
    "del df, eval_days, eval_index_start, evaluate, test, test_index_start, train, values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Input & Output dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(data, n_in, n_out=48):\n",
    "    X, y = list(), list()\n",
    "    in_start = 0\n",
    "    # step over the entire history one time step at a time\n",
    "    for _ in range(len(data)):\n",
    "        # define the end of the input sequence\n",
    "        in_end = in_start + n_input\n",
    "        out_end = in_end + n_out\n",
    "        # ensuring that we have enough data for this instance\n",
    "        if out_end <= len(data):\n",
    "            # need only 1 feature\n",
    "            x_input = data[in_start:in_end, 0]\n",
    "            # reshaping [timestemps, features]\n",
    "            x_input = x_input.reshape((len(x_input), 1))\n",
    "            # it will give [samples, timestemps, features]\n",
    "            X.append(x_input)\n",
    "            # [samples, output]\n",
    "            y.append(data[in_end:out_end, 0])\n",
    "        # move along one time step\n",
    "        in_start += 1\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape:  (33628, 48, 1)\n",
      "Train y shape:  (33628, 48)\n",
      "Evaluation X shape:  (673, 48, 1)\n",
      "Evaluation y shape:  (673, 48)\n",
      "Test X shape:  (1, 48, 1)\n",
      "Test y shape:  (1, 48)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = get_X_y(data=train_scaled, n_in=n_input, n_out=n_output)\n",
    "X_eval, y_eval = get_X_y(data=eval_scaled, n_in=n_input, n_out=n_output)\n",
    "X_test, y_test = get_X_y(data=test_scaled, n_in=n_input, n_out=n_output)\n",
    "print(\"Train X shape: \", X_train.shape)\n",
    "print(\"Train y shape: \", y_train.shape)\n",
    "print(\"Evaluation X shape: \", X_eval.shape)\n",
    "print(\"Evaluation y shape: \", y_eval.shape)\n",
    "print(\"Test X shape: \", X_test.shape)\n",
    "print(\"Test y shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
